\chapter{The Lanczos  method for matrix function approximation}

this means linear forms \( \fAb \) (quadratic forms \( \bfAb \) with quadrature stuff)



\section{Explicit polynomial methods}



\section{The Lanczos method for matrix function approximation}


\note{IDK how much we should put here vs in the OP section.}

\( \vec{Q} := [ \vec{q}_0 , \ldots , \vec{q}_{k-1} ] \)

Moreover, the basis vectors satisfy a symmetric three term recurrence,
$$
\vec{A} \vec{Q} = \vec{Q} \vec{T} + \beta_{k-1} \vec{q}_{k} \vec{e}_k^\T.
$$
Above $\vec{e}_k$ is the $k^\text{th}$ standard basis vector and $\vec{T}$ is symmetric tridiagonal with  diagonals $(\alpha_0, \ldots, \alpha_{k-1})$ and off diagonals $(\beta_0, \ldots, \beta_{k-2})$  which are also computed by the algorithm. I.e., 
\begin{align*}
    {\vec{T}}
    := \begin{bmatrix}
        \alpha_0 & \beta_0 \\
        \beta_0 & \alpha_1 & \ddots \\
        & \ddots & \ddots & \beta_{k-2}\\
        & & \beta_{k-2} & \alpha_{k-1}
    \end{bmatrix}.
\end{align*}

To simplify analysis, it will be useful to consider the recurrence that would obtained if the Lanczos algorithm were run to completion. 
In exact arithmetic, for some $K\leq n$, $\beta_{K-1} = 0$ in which case the algorithm terminates. 
Then the final basis \( \widehat{\vec{Q}} := [ \vec{q}_0 , \ldots , \vec{q}_{K-1} ]\) and symmetric tridiagonal $\widehat{\vec{T}}$ with  diagonals $(\alpha_0, \ldots, \alpha_{K-1})$ and off diagonals $(\beta_0, \ldots, \beta_{K-2})$
satisfy
 a three-term recurrence 
\begin{align*}
    \vec{A} \widehat{\vec{Q}} = \widehat{\vec{Q}} \widehat{\vec{T}}.
\end{align*}
We emphasize that our algorithms \emph{do not} require Lanczos to be run to completion; the introduction of $\widehat{\vec{Q}}$ and $\widehat{\vec{T}}$ is for analysis purposes only.
We note that $\vec{Q} = [\widehat{\vec{Q}}]_{:,:k}$ and $\vec{T} =[\widehat{\vec{T}}]_{:k,:k}$.

Since the columns of $\widehat{\vec{Q}}$ are orthogonal, we have that
\begin{align*}
    \widehat{\vec{T}}
    = \widehat{\vec{Q}}^\T \vec{A} \widehat{\vec{Q}},
\end{align*}
from which we easily see that, after any number of iterations $k$, $\vec{T} = \vec{Q}^\T \vec{A} \vec{Q}$.
Note also that, for any shift $z\in\mathbb{C}$, 
\begin{align*}
    (\vec{A} - z\vec{I}) \widehat{\vec{Q}} = \widehat{\vec{Q}} (\widehat{\vec{T}} - z\vec{I}).
\end{align*}
In other words, the Krylov subspaces generated by $(\vec{A},\vec{b})$ and $(\vec{A}-z\vec{I},\vec{b})$ coincide, and the associated tridiagonal matrices are easily related by a diagonal shift. 
This shift invariance of Krylov subspaces is critical in several low-memory algorithms.




The Lanczos method for matrix function approximation, which we refer to as Lanczos-FA, approximates \( f(\vec{A})\vec{b} \) using   \( \vec{Q}_k \) and \( \vec{T}_k \) as follows: 
\begin{definition}
The \( k \)-th Lanczos-FA approximation to \( f(\vec{A}) \vec{b} \) is defined as
\begin{align*}
    \lan_k(f,\vec{A},\vec{b}) 
    := \vec{Q} f(\vec{T}) \vec{e}_0
    = \vec{Q} f(\vec{T}) \vec{Q}^{\cT} \vec{b},
\end{align*}
where \( \vec{Q} \) and \( \vec{T} \) are produced by the Lanczos method run for \( k \) steps on \( (\vec{A},\vec{b}) \).
For simplicity, we often write \( \lan_k(f) \), since \( \vec{A} \) and \( \vec{b} \) remain fixed for most of this manuscript.
If we are considering the Lanczos algorithm run on a matrix or right hand side different from the given \( \vec{A} \) or \( \vec{b} \), we will use the full notation.
\end{definition}


\subsection{Polynomial error bounds for Lanczos-FA}
\label{sec:polynomial_bounds}



\note{
Add proof:

It is easy to show that \( \lan_k(p) = p(\vec{A})\vec{b} \) for any polynomial \( p \) with \( \deg p < k \); see for example \cite{druskin_knizhnerman_89,saad_92}.
This implies that \( \lan_k(f) = p_{k}(\vec{A})\vec{b} \), where \( p_{k} \) is the degree \(k-1\) polynomial interpolating \( f \) at the eigenvalues of \( \vec{T}_k \). Since eigenvalues of \( \vec{A} \) are often approximated by eigenvalues of \( \vec{T}_k \), this interpolating polynomial is a sensible approximation.
}

More formally, let \( \| \cdot \| \) be any norm induced by a positive definite matrix which commutes with $\vec{A}$; i.e. with the same eigenvectors as $\vec{A}$. Such norms include the 2-norm, the $\vec{A}^2$-norm, and the $\vec{A}$-norm (if $\vec{A}$ is positive definite).
Then \( \| g(\vec{A}) \vec{v} \| \leq \| g(\vec{A}) \|_2 \cdot \| \vec{v} \| \) for any \( g : \mathbb{R}\to\mathbb{R} \), so by the triangle inequality, for any \( p \) with \( \deg p < k \),
\begin{align*}
\| f(\vec{A}) \vec{b} - \lan_k(f) \|
& \leq \| f( \vec{A} ) \vec{b} - p( \vec{A} ) \vec{b} \| + \| p( \vec{A} ) \vec{b} - \lan_k (p) \| + \| \lan_k (p) - \lan_k (f) \| 
\\& = \| (f( \vec{A} ) - p( \vec{A} ) )\vec{b} \| + 0 + \| \vec{Q} ( p( \vec{T} ) - f( \vec{T}_k ) ) \vec{Q}^{\cT} \vec{b} \| 
\\& \le \| f( \vec{A} ) - p( \vec{A} ) \|_2 \cdot \| \vec{b} \| + \| \vec{Q} ( p( \vec{T}_k ) - f( \vec{T} ) ) \vec{Q}^{\cT} \|_2 \cdot \|\vec{b} \|
\\& \le \left( \| f( \vec{A} ) - p( \vec{A} ) \|_2 + \| p(\vec{T} ) - f( \vec{T} ) \|_2 \right) \cdot \| \vec{b} \| . 
\end{align*}
Denote the infinity norm of a scalar function \( h:\R\to\R \) over \( S\subset \R \) by \( \|h\|_S  := \sup_{x\in S}| h(x)| \).
Then, writing the set of eigenvalues of a Hermitian matrix \( \vec{B} \) as \( \Lambda(\vec{B}) \),
\begin{align}
\| f(\vec{A}) \vec{b} - \lan_k(f) \|
    &\leq \min_{\deg p< k} \left( \| f-p \|_{\Lambda(\vec{A})} + \| f - p \|_{\Lambda(\vec{T})} \right) \| \vec{b} \|. \label[ineq]{eqn:triangle_ineq1}
\end{align}
Finally, recalling that \(\mathcal{I}(\vec{A}) := [\lambda_{\text{min}}(\vec{A}), \lambda_{\text{max}}(\vec{A})]\) and using the fact that \( \Lambda(\vec{T}) \subset \mathcal{I}(\vec{A}) \), we obtain the classic bound
\begin{align}
\label[ineq]{eqn:poly_unif}
\| f(\vec{A}) \vec{b} - \lan_k(f) \|_2 
    %&\leq  2 \left( \min_{\deg p<k} \max_{x \in \mathcal{I}(\vec{A})} | f(x) - p(x) | \right) \| \vec{b} \|_2.
    &\leq  2 \min_{\deg p<k} \left( \| f-p \|_{\mathcal{I}(\vec{A})}  \right) \| \vec{b} \|_2.
\end{align}
That is, except for a possible factor of \( 2 \), the error of the Lanczos-FA approximation to \( f( \vec{A} ) \vec{b} \) is at least as good as the best \emph{uniform polynomial approximation to \( f \)} on the interval containing the eigenvalues of \( \vec{A} \).
For arbitrary \( f \), \cref{eqn:poly_unif} remains the standard bound for Lanczos-FA. 
It has been studied carefully and is known to hold to a close degree in finite precision arithmetic \cite{musco_musco_sidford_18}.


\note{reference to bounds for uniform approximation}

\section{Why uniform bounds are insufficient}

However, the uniform error bound of \cref{eqn:poly_unif} is often too loose to accurately predict the performance of Lanczos-FA.
Notably, it depends only on the range of eigenvalues \(\mathcal{I}(\vec{A})\) and not on more fine-grained information like the presence of eigenvalue clusters or isolated eigenvalues, which are known to lead to faster convergence.
The expression in \cref{eqn:triangle_ineq1} is more accurate but it cannot be used as an a priori bound since it involves the eigenvalues of the tridiagonal matrix $\vec T_k$, which depend on $\vec b$. It also cannot be used as a practical a posteriori bound since it involves all  eigenvalues of \( \vec{A} \).

The goal of this paper is to address these limitations. Before doing so, we discuss an example to better illustrate why \cref{eqn:poly_unif} can be  loose as an a priori bound.
It is well known that the eigenvalues of \( \vec{T}_k \) are interlaced by those of \( \vec{A} \); that is, \( \Lambda(\vec{T}_k) \subset \mathcal{I}(\vec{A}) \) and between each pair of eigenvalues of \( \vec{T}_k \) is at least one eigenvalue of \( \vec{A} \).
With this property in mind, define \( \mathcal{J}_k(\vec{A}) \) as the set of all \( k \)-tuples \( \bm{{\mu}} = ( \mu_1 , \ldots , \mu_k ) \in \mathbb{R}^k \) that are interlaced by the eigenvalues of \( \vec{A} \). %; that is, \( \lambda_{\text{min}} ( \vec{A} ) < \min_j \mu_j \), \( \lambda_{\text{max}} ( \vec{A} ) > \max_j \mu_j \), and between each consecutive pair \( \mu_i , \mu_j \) there is at least one eigenvalue of \( \vec{A} \).
Then, we can use \cref{eqn:triangle_ineq1} to write
\begin{align}
\| f(\vec{A}) \vec{b} - \lan_k(f) \| 
% & \leq  \max_{\bm{{\mu}}\in \mathcal{J}_k(\vec{A})} \min_{\deg p<k} \left( \max_{\lambda \in \Lambda(\vec{A})} | f(\lambda) - p(\lambda) | +\max_{j=1, \ldots k} | f(\mu_j) - p(\mu_j) | \right) \| \vec{b} \|. \label[ineq]{eqn:triangle_ineq2} 
 & \leq  \max_{\bm{{\mu}}\in \mathcal{J}_k(\vec{A})} \min_{\deg p<k} \left( \| f-p \|_{\Lambda(\vec{A})} + \| f - p \|_{\bm{{\mu}}} \right) \| \vec{b} \|. \label[ineq]{eqn:triangle_ineq2} 
\end{align}
The bound \cref{eqn:triangle_ineq2} is an a priori error bound, and at least in some special cases, provides more insight than \cref{eqn:poly_unif} in situations where the eigenvalues of $\vec{A}$ are clustered.


\begin{example}
\label{ex:unif_discrete}
Consider \( \vec{A} \) with many eigenvalues uniformly spaced through the interval \( [0,1] \) and a single isolated eigenvalue at \( \kappa > 1 \).
Since the eigenvalues of \( \vec{T}_k \) are interlaced by those of \( \vec A \), there is at most one eigenvalue of \( \vec{T}_k \) between \( 1 \) and \( \kappa \); that is, \( \Lambda( \vec{A} )\cup\Lambda( \vec{T}_k ) \) is contained in \( [0,1] \cup \{ \mu , \kappa \} \) for some \( \mu\in[1,\kappa] \).
We then have
\begin{align}
\label[ineq]{eqn:ex_minimax_bd}
    \| f(\vec{A}) \vec{b} - \lan_k(f) \| 
%\leq \max_{\mu\in[1,\kappa]} \min_{\deg p<k} \left( \max_{\lambda\in[0,1]\cup\{\mu,\kappa\}} | f(\lambda)-p(\lambda)| \right) \| \vec{b} \|.
\leq 2 \max_{\mu\in[1,\kappa]} \min_{\deg p<k} \left( \| f-p \|_{[0,1]\cup\{\mu,\kappa\}} \right) \| \vec{b} \|.
\end{align}
For \( \kappa = 5\), \( f(x) = \exp(-x) \), and \( k=6 \), we use a numerical optimizer to determine that the value  maximizing the right hand side of \cref{eqn:ex_minimax_bd} is  \( \mu^*\approx 4.96\).
In \cref{fig:lanc_poly_approx} we show the error of the Lanczos-FA polynomial along with the optimal uniform polynomial approximations to \( f \) on \( [0,5] \), which contains \( [0,1]\cup\{\mu^*,5\} \).
Here the optimal uniform polynomial approximation is computed by the Remez algorithm.
As expected, the bound from \cref{eqn:ex_minimax_bd}  is significantly better than that from the uniform approximation.

\begin{figure}[ht]
    \includegraphics[width=.97\textwidth]{CIF_imgs/lanc_poly_approx_diff.pdf}

   \caption{
    Comparison of errors of degree 5 polynomial approximations to \( f(x) = \exp(-x) \).
    \emph{Legend}:
    Lanczos-FA approximation for $\vec b$ with equal projection onto all eienvectors of $\vec A$ ({\protect\raisebox{0mm}{\protect\includegraphics[scale=.7]{CIF_imgs/legend/solid_blue.pdf}}}), 
    optimal uniform approximation on \( [0,5] \) ({\protect\raisebox{0mm}{\protect\includegraphics[scale=.7]{CIF_imgs/legend/dash_pink.pdf}}}), 
    optimal uniform approximation on \( [0,1]\cup\{\mu^*,5\} \) ({\protect\raisebox{0mm}{\protect\includegraphics[scale=.7]{CIF_imgs/legend/dashdot_purple.pdf}}}).
    The light vertical lines are the eigenvalues of \( \vec{A} \), while the darker vertical lines are the eigenvalues of \( \vec T_6 \) (the Ritz values).
    \emph{Remarks}: 
    Note that the Lanczos-FA approximation becomes very inaccurate on \( (1,5) \) which allows a smaller error on the eigenvalues of $\vec A$, which is the only error that impacts our approximation to $f(\vec A)\vec b$.
    As a result, the uniform approximation on \( [0,1] \cup \{\mu^*,5\} \) is a much better bound for the Lanczos-FA error than the uniform approximation on \( [0,5] \), which remains equally accurate over the entire interval \([0,5]\). 
    }
    \label{fig:lanc_poly_approx}
\end{figure}
\end{example}




