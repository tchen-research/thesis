\chapter{Matrix-free quadrature}
\label{chap:QF}

    This chapter focuses primarily on quadrature rules for the \emph{weighted} CESM induced by \( \vec{A} \) and a unit vector \( \vec{v} \).
    \begin{definition}
    \label{def:wCESM}
        The weighted CESM \( \Psi : \R\to[0,1] \), induced by \( \vec{A} \) and a unit vector \( \vec{v} \), is defined by
    \begin{equation*}
        \Psi(x)
        = \Psi_{\vec{A},\vec{v}}(x)
        := \vec{v}^\cT \bOne[\vec{A}\leq x] \vec{v}.
%        = x\mapsto \smop{\sum_{i=0}^{n-1}} |\vec{v}^\cT \vec{u}_i|^2 \bOne[ \lambda_i \leq x].
    \end{equation*}
    \end{definition}
    This definition implies that
    \begin{equation*}
        \int f \,\d\Psi = \vec{v}^\cT \fA \vec{v},
    \end{equation*}
    so it is clear that \( \Psi \) is closely related to the task of approximating \( \vec{v}^\cT \fA \vec{v} \). 
    In fact, in this chapter, we take the perspective that Krylov subspace methods for \( \vec{v}^\cT \fA \vec{v} \) are in correspondence with quadrature rules for \( \Psi \).
    Such a perspective was popularized by \cite{golub_meurant_94,golub_meurant_09}

    \begin{remark}
        It is now clear that the Lanczos algorithm \cref{alg:lanczos} is simply the Stieltjes procedure \cref{alg:stieltjes} applied to the weighted CESM \( \Psi \).
        Specifically, \( \vec{q}_i \propto p_i\A\vec{v} \) for \( i=0,1,\ldots, k \) and the tridiagonal matrix \( \vec{T} \) generated by Lanczos is equal to the Jacobi matrix \( \vec{M}(\Psi) \).
    \end{remark}

    %This chapter builds towards developing quadrature approximations to the CESM \( \Phi \) defined in \cref{eqn:CESM} which we discuss in \cref{chap:random_quadrature}

    The chapter title, \emph{matrix-free quadrature} refers to the fact that our approach to constructing quadrature approximations to \( \Psi \) involve matrix-free algorithms; i.e. algorithms which access \( \vec{A} \) only through matrix-vector products.
    While the quadrature rules we study are standard in approximation theory, it is worth noting several critical differences between classical quadrature methods and the algorithm studied in this chapter.
    First, the costs of the algorithms in this paper are determined primarily by the number of matrix-vector products.
    This is because we typically only want to approximate \( \int f\,\d\Psi \) for a single, or perhaps a few, functions. 
    On the other hand, the weight functions which classical quadrature rules approximate never change, so nodes and weights can be precomputed and the dominant cost becomes the cost to evaluate \( f \) at the quadrature nodes.
    Second, while classical weight functions, such as the weight functions for Jacobi or Hermite polynomials, are typically relatively uniform in the interior of the interval of integration, \( \Psi \) may vary wildly from application to application. 
    In some cases \( \Psi \) might resemble the distribution function of a classical weight function whereas in others it might have large gaps, jumps, and other oddities.
    These distinctions are hinted at throughout the chapter and illustrated explicitly in the numerical examples at the end of this chapter.


    Moment based methods for estimating the weighted CESM\footnote{In physics, the ``density'' \( \d\Psi/\d{x} \) is often called the local density of states (local DOS).} have been used in physics for at least half a century.
    Early approaches were based on monomial moments \cite{cyrotlackmann_67,cyrotlackmann_69,ducastelle_cryotlackmann_70,ducastelle_cryotlackmann_71,cryotlackmann_ducastelle_71}, but the use of modified Chebyshev moments \cite{wheeler_blumstein_72} and Lanczos-based approaches \cite{haydock_heine_kelly_pendry_72,haydock_heine_kelly_72,haydock_heine_kelly_75} were soon introduced.


    %\cite{nex_78} integrals


    \section{Extracting moments from a Krylov subspace}
    \label{sec:krylov_moments}


    Bases for the Krylov subspace \( \mathcal{K}_{k+1} = \operatorname{span}\{ \vec{v}, \vec{A}\vec{v}, \ldots, \vec{A}^k \vec{v} \} \) can be computed using \( k \) matrix-vector products with \( \vec{A} \) and contain a wealth of information about the interaction of \( \vec{A} \) with \( \vec{v} \); in particular, they contain the information necessary to compute the moments of \( \Psi \) through degree \( 2k \).
    Indeed, for all \( i,j \geq 0 \),
    \begin{equation*}
        (\vec{A}^j\vec{v})^\cT (\vec{A}^j\vec{v}) = \vec{v}^\cT \vec{A}^{i+j} \vec{v} = \int x^{i+j} \,\d\Psi.
    \end{equation*}
    Note, however, that it is sometimes more straightforward to obtain the moments through degree \( s \), for some \( s\leq 2k \).
    Thus, we will use \( s \) to denote the degree of the maximum moment we compute and \( k \) to denote the number of matrix-vector products used.



    \subsection{Computing modified moments directly}
    \label{sec:moments_direct}
   
    Perhaps the most obvious approach to computing modified moments is to construct the basis \( [ p_0\A\vec{v}, \ldots, p_{k}\A\vec{v}] \) for \( \mathcal{K}_{k+1} \) and then compute 
    \begin{equation*}
        \vec{v}^\cT [ p_0\A\vec{v}, \ldots, p_{k}\A\vec{v}].
    \end{equation*}
    This can be done using \( k \) matrix-vector products and \( O(n) \) storage using the matrix recurrence version of \cref{eqn:poly_three_term}.
    Indeed for all \( i=0,1,\ldots, k-1 \) we have that
    \begin{equation*}
        \vec{A} p_i\A \vec{v} =  \beta_{i-1} p_{i-1}\A\vec{v} + \alpha_i p_i\A\vec{v} + \beta_{i} p_{i+1}\A\vec{v}
    \end{equation*}
    from which we can implement an efficient matrix-free algorithm to compute the modified moments \( \{ m_i \}_{i=0}^{k} \), as shown in \cref{alg:moments}.

    \begin{labelalgorithm}[H]{moments}{get-moments}{Get modified moments of \( \Psi \) wrt. \( \mu \)}
    \begin{algorithmic}[1]
        \Procedure{\thealgorithmname}{$\vec{A}, \vec{v}, k, \mu$}
        \State \( \vec{q}_0 = \vec{v} \), \( m_0 = \vec{v}^\cT\vec{v} \), \( \vec{q}_{-1} = \vec{0} \), \( \beta_{-1} = 0 \)
        \For {\( i=0,1,\ldots, k-1 \)}
            \State \( \vec{q}_{i+1} = \frac{1}{\beta_{i}} \left( \vec{A} \vec{q}_i - \alpha_i \vec{q}_i - \beta_{i-1} \vec{q}_{i-1} \right) \)
            \State \( m_{i+1}  = \vec{v}^\cT \vec{q}_{i+1} \)
        \EndFor
        \State \Return \( \{ m_i \}_{i=0}^{k} \)
    \EndProcedure
    \end{algorithmic}
    \end{labelalgorithm}

    If we instead compute 
    \begin{equation*}
        [ p_0\A\vec{v}, \ldots, p_{k}\A\vec{v}]^\cT
        [ p_0\A\vec{v}, \ldots, p_{k}\A\vec{v}],
    \end{equation*}
    then we have the information required to compute the modified moments though degree \( 2k \).
    However, it is not immediately clear how to do this without the \( O(kn) \) memory required to store a basis for \( \mathcal{K}_{k+1} \).
    It turns out it is indeed generally possible to compute these moments without storing the entire basis, and we discuss a principled approach for doing so using connection coefficients in \cref{sec:connection_coeffs}.

    One case where extracting the moments to degree \( 2k \), without storing a basis for Krylov subspace, is straightforward is when \( \mu = \mu_{a,b}^T \).
    This is because, for all \( i \geq 0 \), the Chebyshev polynomials satisfy the identities
    \begin{equation*}
        T_{2i} = 2 (T_i)^2 - 1 
        ,\qquad
        T_{2i+1} = 2 T_i T_{i+1} - x. 
    \end{equation*}
    Thus, using the recurrence for the Chebyshev polynomials and their relation to the orthogonal polynomials with respect to \( \mu_{a,b}^T \), we obtain \cref{alg:cheb_moments}.
    This algorithm is well-known in papers on the kernel polynomial method are variants; see for instance \cite{skilling_89,silver_roder_94,weisse_wellein_alvermann_fehske_06,hallman_21}.% and is based on related ideas from \cite{}.

    \begin{remark}   
    The Chebyshev polynomials grow rapidly outside of the interval \( [-1,1] \).
    Therefore, if the spectrum of \( \vec{A} \) extends beyond this interval, then computing the Chebyshev polynomials in \( \vec{A} \) may suffer from numerical instabilities.
        Instead, the distribution function \( \mu_{a,b}^T \) and corresponding orthogonal polynomials should be used for some choice of \( a \) and \( b \) with \( \mathcal{I} \subset [a,b] \).

    %then \( \vec{A} \) can be scaled and shifted so the eigenvalues are in \( [-1,1] \) using the transform
    %\begin{equation*}
    %    \vec{A} \mapsto \frac{2}{b-a} \left( \vec{A} - \frac{a+b}{2}\vec{I} \right).
    %\end{equation*}
    %    This transformed matrix doesn't need to be computed explicitly since matrix-vector products with a vector \( \vec{x} \) can be written as a linear combination of \( \vec{A}\vec{x} \) and \( \vec{x} \); e.g. \( \frac{2}{b-a} ( \vec{A} - \frac{a+b}{2}\vec{I} ) \vec{x} = \frac{2}{b-a}( \vec{A}\vec{x} - \frac{a+b}{2} \vec{x} )\).
    \end{remark}


    \begin{labelalgorithm}[H]{cheb_moments}{get-Chebyshev-moments}{Get modified moments of \( \Psi \) wrt. \( \mu_{a,b}^T \)}
    \begin{algorithmic}[1]
        \Procedure{\thealgorithmname}{$\vec{A}, \vec{v}, k, a,b$}
        \State \( \vec{q}_0 = \vec{v} \), \( m_0 = \vec{q}_0^\cT \vec{q}_0 \)
        \State \( \vec{q}_1 = \frac{2}{b-a}(\vec{A} \vec{q}_0 - \frac{a+b}{2} \vec{q}_0) \), \( m_1 =  \sqrt{2} \vec{q}_0^\cT \vec{q}_1 \)
        \For {\( i=1,2,\ldots, k-1 \)}
            \State \( m_{2i}  = \sqrt{2} (2\vec{q}_i^\cT \vec{q}_{i} - m_0) \)
            \State \( \vec{q}_{i+1} = 2 \frac{2}{b-a} (\vec{A} \vec{q}_i - \frac{a+b}{2} \vec{q}_i) - \vec{q}_{i-1} \)
            \State \( m_{2i+1}  = \sqrt{2} (2\vec{q}_i^\cT \vec{q}_{i+1}) - m_1 \)
        \EndFor
        \State \( m_{2k} = \sqrt{2}( 2 \vec{q}_k^\cT\vec{q}_k - m_0) \)
        \State \Return \( \{ m_i \}_{i=0}^{2k} \)
    \EndProcedure
    \end{algorithmic}
    \end{labelalgorithm}


    \subsection{Connection coefficients to compute more modified moments}
    \label{sec:connection_coeffs}

    We now discuss how to use connection coefficients to compute the modified moments of \( \Psi \) with respect to \( \mu \) given knowledge of either
    (i) the modified moments of \( \Psi \) with respect to some distribution \( \nu \) or 
    (ii) the tridiagonal matrix computed using \cref{alg:lanczos}.
    Much of our discussion on connection coefficients is based on \cite{webb_olver_21}; see also \cite{fischer_golub_91}.

    \begin{definition}
        \label[definition]{def:connection_coeffs}
        The connection coefficient matrix \( \vec{C} = \vec{C}_{\mu\to\nu} \) is the upper triangular matrix representing a change of basis between the orthogonal polynomials \( \{ p_i \}_{i=0}^{\infty} \) with respect to \( \mu \) and the orthogonal polynomials \( \{ q_i \}_{i=0}^{\infty} \) with respect to \( \nu \), whose entries satisfy,
        \begin{equation*}
            p_s = [\vec{C}]_{0,s} q_0 +  [\vec{C}]_{1,s} q_1 +  \cdots + [\vec{C}]_{s,s} q_s.
        \end{equation*}
    \end{definition}

    \Cref{def:connection_coeffs} implies that, for all \( i = 0,1, \ldots, s \),
    \begin{equation*}
        m_i =
        \int p_i \,\d\Psi
        = 
        \smop{\sum_{j=0}^{i}} [\vec{C}]_{j,i} \int q_j \,\d\Psi
        =
        \smop{\sum_{j=0}^{i}} [\vec{C}]_{j,i} n_j
    \end{equation*}
    where \( \{n_i\}_{i=0}^{s} \) are the modified moments of \( \Psi \) with respect to \( \nu \).
    Thus, we can easily obtain the modified moments \( \{ m_i \}_{i=0}^{s} \) of \( \Psi \) with respect to \( \mu \) from the modified moments of \( \Psi \) with respect to \( \nu \).
    In particular, if \( \vec{m} \) and \( \vec{n} \) denote the vectors of modified moments, then \( \vec{m} = \vec{C}^\rT \vec{n} \).

    Moreover, in the special case that \( \nu \) has the same moments as \( \Psi \) through degree \( s \), so, for any \( j\geq 0 \), 
    \begin{equation*}
        n_j = \int q_j \,\d\Psi 
        = \int q_j \,\d\nu
        = \int q_0 q_j \,\d\nu 
        = \bOne[j = 0] .
    \end{equation*}
    Therefore, the modified moments of \( \Psi \) (with respect to \( \mu \)) through degree \( s \) can be computed by
    \begin{equation*}
        m_i=
        \int p_i \,\d\Psi = 
        [ \vec{C}]_{0,i}.
    \end{equation*}

    In order to use the above expressions, we must compute the connection coefficient matrix.
    \Cref{def:connection_coeffs} implies that for all \( i\leq j \), the entries of the connection coefficient matrix are given by
    \begin{equation*}
        [\vec{C}]_{i,j} = \int q_i p_j \,\d\nu.
    \end{equation*}    
    Unsurprisingly, then, the entries of the connection coefficient matrix \( \vec{C} = \vec{C}_{\mu\to\nu} \) can be obtained by a recurrence relation.
    \begin{proposition}[{\cite[Corollary 3.3]{webb_olver_21}}]
    \label[proposition]{thm:connection_coefficient_recurrence}
        Suppose the Jacobi matrices for \( \mu \) and \( \nu \) are respectively given by
        \begin{equation*}
            \vec{M}(\mu) =
        \begin{bmatrix}
            \alpha_0 & \beta_0 \\
            \beta_0 & \alpha_1 & \beta_1 & \phantom{\ddots}\\
            &\beta_1 & \alpha_2 & \ddots \\
            &&\ddots & \ddots \\
        \end{bmatrix}
        ,\qquad
            \vec{M}(\nu) =
        \begin{bmatrix}
            \gamma_0 & \delta_0 \\
            \delta_0 & \gamma_1 & \delta_1 & \phantom{\ddots}\\
            &\delta_1 & \gamma_2 & \ddots \\
            &&\ddots & \ddots \\
        \end{bmatrix}.
        \end{equation*}

        Then the entries of \( \vec{C} = \vec{C}_{\mu\to\nu} \) satisfy, for \( i,j \geq 0 \), the following recurrence:
    \begin{align*}
        [\vec{C}]_{0,0} &= 1 
        \\
        [\vec{C}]_{0,1} &= (\gamma_0 - \alpha_0)/\beta_0
        \\
        [\vec{C}]_{1,1} &= \delta_0 / \beta_0 
        \\
        [\vec{C}]_{0,j} &= ((\gamma_0 - \alpha_{j-1}) [\vec{C}]_{0,j-1} + \delta_0 [\vec{C}]_{2,j-1} - \beta_{j-2} [\vec{C}]_{0,j-2} ) / \beta_{j-1}
        \\
        [\vec{C}]_{i,j} &= (\delta_{i-1} [\vec{C}]_{i-1,j-1} + (\gamma_i - \alpha_{j-1}) [\vec{C}]_{i,j-1} + \delta_i[\vec{C}]_{i+1,j-1} - \beta_{j-2} [ \vec{C}]_{i,j-2}) / \beta_{j-1}.
    \end{align*}
    \end{proposition}

    \Cref{thm:connection_coefficient_recurrence} yields a natural algorithm for computing the connection coefficient matrix \( \vec{C}_{\mu\to\nu} \).
    This algorithm is shown as \cref{alg:connection_coeffs}.
    Note that \( \vec{C} \) is, by definition, upper triangular, so \( [\vec{C}]_{i,j}  = 0 \) whenever \( i > j \).
    We remark that for certain cases, particularly transforms between the Jacobi matrices of classical orthogonal polynomials, faster algorithms are known \cite{townsend_webb_olver_17}.
    We do not focus on such details in this paper as the cost of products with \( \vec{A} \) is typically far larger than the cost of computing \( \vec{C}_{\mu\to\nu} \).

    \begin{labelalgorithm}[H]{connection_coeffs}{get-connection-coeffs}{Get connection coefficients}
    \begin{algorithmic}[1]
        %\Procedure{\thealgorithmname}{$[\vec{M}(\mu)]_{:k_{m_1},:k_{m_2}},[\vec{M}(\nu)]_{:k_{n_1},:k_{n_2}}$}
        \Procedure{\thealgorithmname}{$\mu,k_{\mu},k_{\mu}',\nu,k_{\nu},k_{\nu}'$}
        \State \( [\vec{C}]_{0,0} = 1 \), \( [\vec{C}]_{i',j'} = 0 \) if \( i' > j' \) or \( j' = -1 \)
        \For {$j=1,2,\ldots,\min(k_{\mu}',k_{\nu}+k_{\nu}')$}
        \For {$i=0,1,\ldots,\min(j,k_{\nu}+k_{\nu}'-j)$}
%        \State {$[\vec{C}]_{i,j} = \frac{1}{\beta_{j-1}}(\delta_{i-1} [\vec{C}]_{i-1,j-1} + (\gamma_i - \alpha_{j-1}) [\vec{C}]_{i,j-1} + \delta_i[\vec{C}]_{i+1,j-1} - \beta_{j-2} [ \vec{C}]_{i,j-2}) $}
        \State 
        \(
        \begin{aligned}%
            \relax[\vec{C}]_{i,j} = (&\delta_{i-1} [\vec{C}]_{i-1,j-1} + (\gamma_i - \alpha_{j-1}) [\vec{C}]_{i,j-1} 
            \\[-.6em]&\hspace{3em}+ \delta_i[\vec{C}]_{i+1,j-1} 
            - \beta_{j-2} [ \vec{C}]_{i,j-2}) / \beta_{j-1} 
        \end{aligned}\)
        \EndFor
        \EndFor
        \State \Return \( \vec{C} = \vec{C}_{\mu\to\nu} \)
    \EndProcedure
    \end{algorithmic}
    \end{labelalgorithm}

    \begin{remark}
        From \cref{thm:connection_coefficient_recurrence} it is not hard to see that \( [\vec{C}]_{:k,:k} \) can be computed using \( [\vec{M}(\nu)]_{:k,:k} \) and \( [\vec{M}(\mu)]_{:k,:k} \). 
        Moreover, \( [\vec{C}]_{0,:2k+1} \) can be computed using \( [\vec{M}(\nu)]_{:k+1,:k} \) and \( [\vec{M}(\mu)]_{:2k,:2k} \). 
        In general \( \vec{M}(\mu) \) will be known fully, and in such cases, the modified moments through degree \( 2k \) can be computed using the information generated by Lanczos run for \( k \) iterations.
    \end{remark}

    We can use \cref{alg:connection_coeffs} in conjunction with \cref{alg:cheb_moments} and \cref{alg:lanczos} to compute modified moments with respect to \( \mu \).
    This is shown in \cref{alg:moments_cheb} and \cref{alg:moments_lanc} respectively.

    \begin{labelalgorithm}[H]{moments_cheb}{get-moments-from-Cheb}{Get modified moments wrt. \( \mu \) of weighed CESM (via Chebyshev moments)}
    \begin{algorithmic}[1]
        \Procedure{\thealgorithmname}{$\vec{A}, \vec{v}, s, \mu, a, b$}
        \State \( k = \lceil s/2 \rceil \)
        \State \( \{ n_i \}_{i=0}^{2k} = \text{\ref{alg:name:cheb_moments}}(\vec{A},\vec{v}, k,a,b)  \)
        \State \( \vec{C} = \text{\ref{alg:name:connection_coeffs}}([\vec{M}(\mu)]_{:2k,:2k},[\vec{M}(\mu_{a,b}^T)]_{:2k,:2k}) \)
        \For{\( i=0,1,\ldots, s \)}
            \State \( m_i = \sum_{j=0}^{i} [\vec{C}]_{j,i} n_j \)
        \EndFor
        \State \Return \( \{ m_i \}_{i=0}^{s} \)
    \EndProcedure
    \end{algorithmic}
    \end{labelalgorithm}


    \begin{labelalgorithm}[H]{moments_lanc}{get-moments-from-Lanczos}{Get modified moments wrt. \( \mu \) of weighed CESM (via Lanczos)}
    \begin{algorithmic}[1]
        \Procedure{\thealgorithmname}{$\vec{A}, \vec{v}, s, \mu$}
        \State \( k = \lceil s/2 \rceil \)
        \State \( [\vec{T}]_{i:k+1,:k} = \text{\ref{alg:name:lanczos}}(\vec{A},\vec{v}, k)  \)
        \State \( \vec{C} = \text{\ref{alg:name:connection_coeffs}}([\vec{M}(\mu)]_{:2k,:2k},[\vec{T}]_{:k+1,:k}) \)
        \For{\( i=0,1,\ldots, s \)}
            \State \( m_i = [\vec{C}]_{0,i} \)
        \EndFor
        \State \Return \( \{ m_i \}_{i=0}^{s} \)
    \EndProcedure
    \end{algorithmic}
    \end{labelalgorithm}
    
    \begin{remark}
    Given the modified moments of \( \Psi \) with respect to \( \mu \), the tridiagonal matrix produced by \cref{alg:lanczos} can itself be obtained \cite{sack_donovan_71}.
    This is quite similar to \( s \)-step Lanczos methods designed to reduce communication on distributed memory computers.
    However, if implemented naively, such methods can be even more susceptible to the effects of finite precision arithmetic than the regular Lanczos method \cite{carson_demmel_15,carson_20}, so special care must be taken when implementing such an algorithm.
    \end{remark}


\section{Quadrature approximations for weighted spectral measures}
\label{sec:quadrature}

We now discuss how to use the information extracted by the algorithms in the previous section to obtain quadrature rules for the weighted CESM \( \Psi \).
We begin with a discussion on quadrature by interpolation in \cref{sec:iq} followed by a discussion on Guassian quadrature in \cref{sec:gq} and quadrature by approximation in \cref{sec:aq}.
Finally, in \cref{sec:damped}, we describe how damping can be used to ensure the positivity of quadrature approximations.

\subsection{Quadrature by interpolation}
\label{sec:iq}

Our first class of quadrature approximations for \( \Psi \) is the degree \( s \) quadrature by interpolation \( \qq[i]{\Psi}{s} \) (i.e. $\circ = \textrm{i}$) which is defined by the relation
\begin{equation}
    \label{eqn:interp_f}
    \int f \,\d\qq[i]{\Psi}{s}
    := \int \ff[i]{f}{s} \,\d\Psi,
\end{equation}
where \( \ff[i]{f}{s} \) is the degree \( s \) polynomial interpolating a function \( f \) at the zeros \( \{\theta_{j}^{(s+1)}\}_{j=0}^{s} \) of \( p_{s+1} \), the degree $s+1$ orthogonal polynomial with respect to \( \mu \).
\Cref{eqn:interp_f} implies that
\begin{equation*}
    \qq[i]{\Psi}{s} = \smop{\sum_{j=0}^{s}} \omega_{j}^{} \bOne [\theta_{j}^{(s+1)} \leq x ]
\end{equation*}
where the weights \( \{ \omega_{j}^{} \}_{j=0}^{s} \) are chosen such that the moments of \( \qq[i]{\Psi}{s} \) agree with those of \( \Psi \) through degree \( s \).


One approach to doing this is by solving the Vandermonde-like linear system of equations%
\begin{equation}
    \label{eqn:vandermonde}
    \begin{bmatrix}
        \vphantom{\Big(} 
        p_0(\theta_{0}^{(s+1)}) & \cdots & p_0 (\theta_{s}^{(s+1)}) \\
        \vphantom{\Big(} 
        \vdots & & \vdots \\
        \vphantom{\Big(} 
        p_{s}(\theta_{0}^{(s+1)}) & \cdots & p_{s}(\theta_{s}^{(s+1)}) \\
    \end{bmatrix}
    \begin{bmatrix}
        \vphantom{\Big(}\omega_{0}^{} \\ 
        \vphantom{\Big(} 
        \vdots \\ \vphantom{\Big(} \omega_{s}^{}
    \end{bmatrix}
    =
    \begin{bmatrix}
        \vphantom{\Big(} \int p_0 \,\d\Psi \\ 
        \vphantom{\Big(} 
        \vdots \\ \vphantom{\Big(} \int p_{s} \,\d\Psi
    \end{bmatrix}.
\end{equation}
which we denote by \( \vec{P} \vec{\omega} = \vec{m} \). 
This will ensure that polynomials of degree up to \( s \) are integrated exactly. 

While it is not necessary to restrict the test polynomials to be the orthogonal polynomials \( \{ p_i \}_{i=0}^{\infty} \) with respect to \( \mu \) nor the interpolation nodes to be the zeros \( \{ \theta_j^{(s+1)} \}_{j=0}^{s} \) of \( p_{s+1} \), doing so has several advantages.
If arbitrary polynomials are used, the matrix \( \vec{P} \) may be exponentially ill-conditioned; i.e. the condition number of the matrix could grow exponentially in \( s \).
This can cause numerical issues with solving \( \vec{P} \vec{\omega} = \vec{m} \).
If orthogonal polynomials are used, then as in \cref{eqn:jacobi_evec} we see that the columns of \( \vec{P} \) are eigenvectors of the Jacobi matrix \( \vec{M} \).
Since \( \vec{M} \) is symmetric, this implies that \( \vec{P} \) has orthogonal columns; i.e. \( \vec{P}^\cT\vec{P} \) is diagonal.
%This means that the columns of the matrix will not be near linearly independent. 
Therefore, we can easily apply \( \vec{P}^{-1} \) through a product with \( \vec{P}^\cT \) and an appropriately chosen diagonal matrix.
In particular, if \( \vec{S} \) is the orthonormal matrix of eigenvectors, then \( \vec{P} = \vec{S} \operatorname{diag}([\vec{S}]_{0,:})^{-1} \) so that \( \vec{P}^{-1} = \operatorname{diag}([\vec{S}]_{0,:})\vec{S}^\cT \).
This yields \cref{alg:iq}.

\begin{labelalgorithm}[H]{iq}{get-IQ}{Quadrature by interpolation}
\begin{algorithmic}[1]
    \Procedure{\thealgorithmname}{$\{m_i\}_{i=0}^{s},\mu$}
    \State \( \vec{\theta},\vec{S} = \Call{eig}{[\vec{M}(\mu)]_{:s+1,:s+1}} \) \Comment{Eigenvectors normalized to unit length}
    \State \( \vec{\omega} = \operatorname{diag}([\vec{S}]_{0,:})\vec{S}^\cT \vec{m} \) 
%    \Comment{Eigenvectors normalized so the first row is 1}
%    \State \( \bm{\omega} = \Call{solve}{\vec{P},\vec{m}} \) 
    \State \Return \( \qq[i]{\Psi}{s} = \sum_{j=0}^{s} [\vec{\omega}]_{j} \bOne[ [\vec{\theta}]_{j} \leq x] \)
\EndProcedure
\end{algorithmic}
\end{labelalgorithm}

\begin{remark}   
In certain cases, such as \( \mu = \mu_{a,b}^T \), \( \vec{P}^{-1} \) can be applied quickly and stably using fast transforms, such as the discrete cosine transform, without ever constructing \( \vec{P} \).
\end{remark}



\subsection{Gaussian quadrature}
\label{sec:gq}

While interpolation-based quadrature rules supported on \( k \) nodes do not, in general, integrate polynomials of degree higher than \( k-1 \) exactly, if we allow the nodes to be chosen adaptively we can do better.
The degree \( 2k-1 \) Gaussian quadrature rule \( \qq[g]{\Psi}{2k-1} \) for \( \Psi \) is obtained by constructing an quadrature by interpolation rule at the roots \( \{ \theta_{i}^{(k)} \}_{i=1}^{k} \) of the degree \( k \) orthogonal polynomial \( p_{k} \) of \( \Psi \) (i.e. by taking \( \mu = \Psi \)).
\begin{theorem}
If \( p \) is any polynomial of degree at most \( 2k-1 \), then 
\begin{equation*}
    \int p\,\d\Psi = \int p \,\d\qq[g]{\Psi}{2k-1}.
\end{equation*}
I.e., the Gaussian quadrature rule integrates polynomials of degree \( 2k-1 \) exactly.
\end{theorem}

\begin{proof}
We can decompose \( p \) as
\begin{equation*}
    p = q  p_k + r
\end{equation*}
where \( q \) and \( r \) are each polynomials of degree at most \( k-1 \).
Since \( p_k \) is the \( k \)-th orthogonal polynomial with respect to \( \mu = \Psi \), it is orthogonal to all polynomials of lower degree, including \( q \).
Thus,
\begin{equation*}
    \int p \,\d\Psi
    = \int q p_k \,\d\Psi + \int r\,\d \Psi
    = \int r \,\d\Psi.
\end{equation*}
    On the other hand, since the interpolation nodes \( \{\theta_j^{(k)}\}_{j=0}^{k-1} \) are the roots of  \( p_k \),
\begin{equation*}
    \int p \,\d\qq[i]{\Psi}{k-1}
    = \smop{\sum_{j=0}^{k-1}} \omega_{j} \big( q(\theta_{j}^{(k)} ) p_k(\theta_{j}^{(k)} ) + r(\theta_{j}^{(k)} )\big)
    = \smop{\sum_{j=0}^{k-1}} \omega_{j} r(\theta_{j}^{(k)} )
    = \int r \,\d\qq[i]{\Psi}{k-1}.
\end{equation*}
Since the quadrature rule \( \qq[i]{\Psi}{k} \) is interpolatory of degree \( k \), this implies
\begin{equation*}
    \int p \,\d\Psi
    = \int p \,\d\qq[i]{\Psi}{k-1}.
    \qedhere
\end{equation*}
\end{proof}

Because the polynomials \( \{ p_i \}_{i=0}^{\infty} \) are orthogonal with respect to the probability distribution \( \Psi \) function,
we have that, for all \( i\geq 0 \),
\begin{equation*}
    m_i = \vec{v}^\cT p_i(\vec{A})\vec{v} 
    =
    \int p_i p_0 \,\d\Psi(\vec{A},\vec{v})
    = \bOne[i=0].
    %\begin{cases}
    %    1 & i=0 \\
    %    0 & i>0
    %\end{cases}.
\end{equation*}
This means the right hand side \( \vec{m} \) of \cref{eqn:vandermonde} is the first canonical unit vector \( \vec{e}_0 = [1,0,\ldots, 0]^\cT \).
Thus, as in \cref{alg:iq}, \( \vec{\omega} = \operatorname{diag}([\vec{S}]_{0,:}) [\vec{S}]_{0,:}\); that is, the quadrature weights are the squares of the first components of the unit length eigenvectors of \( [\vec{T}]_{:k,:k} \).
We then arrive at \cref{alg:gq} for obtaining a Gaussian quadrature rule for \( \Psi(\vec{A},\vec{v}) \) from the tridiagonal matrix \( [\vec{T}]_{:k,:k} \) generated by \cref{alg:lanczos}.

\begin{labelalgorithm}[H]{gq}{get-GQ}{Gaussian quadrature}
\begin{algorithmic}[1]
    \Procedure{\thealgorithmname}{$[\vec{T}]_{:k,:k}$}
%    \State \( [\vec{T}]_{:s,:s} = \textsc{Lanczos}(\vec{A},\vec{v},s) \)
    \State \( \vec{\theta}, \vec{S} = \textsc{eig}([\vec{T}]_{:k,:k}) \) \Comment{Eigenvectors normalized to unit length}
    \State \( \vec{\omega} = \operatorname{diag}([\vec{S}]_{0,:}) [\vec{S}]_{0,:} \)
    \State \Return \( \qq[g]{\Psi}{2k-1} = \sum_{j=0}^{k-1} [\vec{\omega}]_j \bOne[[\vec{\theta}]_j \leq x] \)
\EndProcedure
\end{algorithmic}
\end{labelalgorithm}

\begin{remark}
    To construct a Gaussian quadrature rule, the three term recurrence for the orthogonal polynomials of \( \Psi \) must be determined. 
    Thus, the main computational cost is computing the tridiagonal matrix giving this recurrence. 
    However, due to orthogonality, we know that all but the degree zero modified moments are zero and do not need to compute the moments.
    This is in contrast to other schemes where the polynomial recurrence is known but the modified moments must be computed.
\end{remark}



\subsection{Quadrature by approximation}
\label{sec:aq}

Rather than defining a quadrature approximation using an interpolating polynomial, we might instead define an approximation \( \qq[a]{\Psi}{s} \) by the relation
\begin{equation*}
    %\label{eqn:approx_f}
    \int f\d\qq[a]{\Psi}{s}
    := \int \ff[a]{f}{s} \,\d\Psi
\end{equation*}
where \( \ff[a]{f}{s} \) is the projection of \( f \) onto the orthogonal polynomials with respect to \( \mu \) through degree \( s \) in the inner product \( \langle \cdot, \cdot\rangle_\mu\: \).
That is,
%Explicitly, \( \ff[a]{f}{s} \) is given by 
\begin{equation*}
    \ff[a]{f}{s}
    := \:\smop{\sum_{i=0}^{s}} \langle f, p_i \rangle_\mu\: p_i
    = \:\smop{\sum_{i=0}^{s}} \left(\int f p_i\d\mu \right) p_i.
\end{equation*}

Expanding the integral of \(  \ff[a]{f}{s} \) against \( \Psi \), 
\begin{equation*}
    \int \ff[a]{f}{s} \,\d\Psi
%    &= \int \sum_{i=0}^{s} \langle f, p_i \rangle_\mu\: p_i \,\d\Psi
%    = \sum_{i=0}^{s} \langle f, p_i \rangle_\mu\: \int p_i \,\d\Psi
    = \int \smop{\sum_{i=0}^{s}} \left(\int f p_i \,\d\mu\right) p_i \,\d\Psi
    = \int f  \bigg( \smop[r]{\sum_{i=0}^{s}} \left(\int p_i \,\d\Psi\right) p_i \bigg) \,\d\mu.
\end{equation*}
This implies 
\begin{equation*}
%    \label{eqn:kpm_expanded}
    \frac{\d\qq[a]{\Psi}{s}}{\d\mu} 
    = \:\smop{\sum_{i=0}^{s}} \left( \int p_i \,\d\Psi \right) p_i 
    = \:\smop{\sum_{i=0}^{s}} m_i p_i 
\end{equation*}
where \( \d\qq[a]{\Psi}{s}/\d\mu \) is the Radon--Nikodym derivative of \( \qq[a]{\Psi}{s} \) with respect to \( \mu \).


Supposing%
\footnote{If \( \Psi = \Psi(\vec{A},\vec{v}) \) then \( \Psi \) is not absolutely continuous with respect to the Lebesgue measure (or any equivalent measure) so the Radon--Nikodym derivative does not exist. 
However, there are absolutely continuous distribution distributions with the same modified moments as \( \Psi \) up to arbitrary degree, so conceptually one can use such a distribution instead.}
that the Radon--Nikodym derivative \( \d\Psi/\d\mu \) exists, we observe 
\begin{equation*}
    m_i = \int p_i \,\d\Psi
    = \int p_i \frac{\d\Psi}{\d\mu} \,\d\mu
\end{equation*}
is the \( \mu \)-projection of \( \d\Psi/\d\mu \) onto \( p_i \) for \( i=0,1,\ldots, s \). 
Thus \( \d\Psi/\d\mu \) is approximated in a truncated orthogonal polynomial series as
\begin{equation*}
    \smop{\sum_{i=0}^{s}} \left( \int p_i \frac{\d\Psi}{\d\mu} \,\d\mu \right) \: p_i
    =
    \smop{\sum_{i=0}^{s}} \left( \int p_i \,\d\Psi \right) \: p_i.
\end{equation*}
This means the density \( \d\qq[a]{\Psi}{s}/\d{x} \) is, at least formally, the polynomial approximation to the Radon--Nikodym derivative \( \d\Psi/\d\mu \) times the density \( \d\mu/\d{x} \); i.e. \( \d\qq[a]{\Psi}{s}/\d{x} = \ff[a]{\d\Psi/\d\mu}{s} \).

We can obtain an approximation to the density \( \d\Psi/\d{x} \) by using the density \( \d\mu/\d{x} \) and the definition of the Radon--Nikodym derivative: 
\begin{equation*}
    \frac{\d\qq[a]{\Psi}{s}}{\d{x}} 
    =  \frac{\d\qq[a]{\Psi}{s}}{\d\mu}  \frac{\d\mu}{\d{x}}
    = \frac{\d\mu}{\d{x}} \smop{\sum_{i=0}^{s}} m_i p_i.
\end{equation*}
Converting this ``density'' to a distribution gives the approximation \( \qq[a]{\Psi}{s} \) shown in \cref{alg:aq}. 
%is the well sometimes called the KPM approximation to the empirical spectral measure\footnote{The empirical spectral measure is commonly called ``density of states'' in the physical sciences.} \( \d\Phi/\d{x} \).

\begin{labelalgorithm}[H]{aq}{get-AQ}{Quadrature by approximation}
\begin{algorithmic}[1]
    \Procedure{\thealgorithmname}{$\{ m_i \}_{i=0}^{s}, \mu$}%[\vec{M}]_{:s+1,:s+1}$}
    \State \Return \( \qq[a]{\Psi}{s} = \left( x \mapsto \sum_{i=0}^{s} m_i \int_{-\infty}^{x} p_i \,\d\mu \right) \)
\EndProcedure
\end{algorithmic}
\end{labelalgorithm}

\begin{remark}
    When \( \mu = \mu_{a,b}^T \), \( {\d\qq[a]{\Psi}{s}} / {\d\mu} \) can be evaluated quickly at Chebyshev nodes by means of the discrete cosine transform.
    This allows the density \( {\d\qq[a]{\Psi}{s}} / {\d{x}} \) to be evaluated quickly at these points.
\end{remark}


\subsubsection{Evaluating spectral sums and the relation to quadrature by interpolation}

We have written the output of \cref{alg:aq} as a distribution function for consistency.
However, note that 
\begin{equation*}
    \int f \,\d \qq[a]{\Psi}{s}
    = \smop{\sum_{i=0}^{s}} m_i \int f  p_i \,\d\mu.
\end{equation*}
Thus, if used for the task of spectral sum approximation, the distribution function \( \qq[a]{\Psi}{s} \) need not be computed. 
Rather, the \( \mu \)-projections of \( f \) onto the orthogonal polynomials with respect to \( \mu \) can be used instead.
In many cases, the values of these projections are known analytically, and even if they are unknown, computing them is a scalar problem independent of the matrix size \( n \).


A natural approach to computing the \( \mu \)-projections of \( f \) numerically is to use a quadrature approximation for \( \mu \).
Specifically, we might use the \( d \)-point Gaussian quadrature rule \( \qq[g]{\mu}{2d-1} \) for \( \mu \) to approximate \( \int f p_i \,\d\mu \).
This gives us the approximation 
\begin{equation*}
    \smop{\sum_{i=0}^{s}} m_i \int f p_i \,\d\mu
    \approx \smop{\sum_{i=0}^{s}}  m_i \int f p_i \,\d \qq[g]{\mu}{2s+1}
    = \smop{\sum_{i=0}^{s}}  m_i \smop{\sum_{j=0}^{d-1}} \omega_{i}^{(d)} f(\theta_{j}^{(d)}) p_i(\theta_j^{(d)})
\end{equation*}
where \( \{ \omega_i^{(d)} \}_{i=0}^{d-1} \) are the Gaussian quadrature weights for \( \mu \).

Similar to above, denote by \( \vec{P} \) the \( d\times d \) Vandermonde-like matrix of orthogonal polynomials with respect to \( \mu \) evaluated at the zeros of \( p_{d+1} \).
If \( \vec{S} \) is the orthonormal matrix of eigenvectors of the \( d\times d \) Jacobi matrix \( [\vec{M}]_{:d,:d} \) for \( \mu \), then recall that the Gaussian quadrature weights \( \vec{\omega}^{(d)} \) are given by \( \operatorname{diag}([\vec{S}]_{0,:s+1})([\vec{S}]_{:s+1,:})^\cT \).
This yields \cref{alg:aaq}.

\begin{remark}
In the case \( d=s+1 \), \cref{alg:aaq} is equivalent to \cref{alg:iq}.
\end{remark}

\begin{labelalgorithm}[H]{aaq}{get-aAQ}{Approximate quadrature by approximation}
\begin{algorithmic}[1]
    \Procedure{\thealgorithmname}{$\{m_i\}_{i=0}^{s},d,\mu$}%[\vec{M}]_{:d,:d}$}
    \State \( \vec{\theta},\vec{S} = \Call{eig}{[\vec{M}(\mu)]_{:d,:d}} \) \Comment{Eigenvectors normalized to unit length}
    \State \( \vec{\omega} = \operatorname{diag}([\vec{S}]_{0,:s+1})([\vec{S}]_{:s+1,:})^\cT \vec{m} \) 
%    \Comment{Eigenvectors normalized so the first row is 1}
%    \State \( \bm{\omega} = \Call{solve}{\vec{P},\vec{m}} \) 
    \State \Return \( \qq[i]{\Psi}{s} = \sum_{j=0}^{k-1} [\vec{\omega}]_{j} \bOne[[\vec{\theta}]_{j} \leq x]  \)
\EndProcedure
\end{algorithmic}
\end{labelalgorithm}



\subsection{Positivity by damping and the kernel polynomial method}
\label{sec:damped}

While it is clear that that the Gaussian quadrature \( \qq[g]{\Psi}{s} \) is a non-negative probability distribution function, neither \( \qq[i]{\Psi}{s} \) nor \( \qq[a]{\Psi}{s} \) are guaranteed to be weakly increasing.
We now discuss how to use damping to enforce positivity.

Towards this end, define the damping kernel,
\begin{equation*}
    P_x(y) 
    %= \sum_{i=0}^{s} \sum_{j=0}^{s} \rho_{i,j} p_i(x) p_j(y)
    = \smop{\sum_{i=0}^{s}} \rho_{i} p_i(x) p_i(y)
\end{equation*}
where \( \{ \rho_i \}_{i=0}^{s} \) are \emph{damping coefficients} as in \cref{def:damping}.
Then the damped interpolant \( \ff[d-i]{f}{s} \) and approximant \( \ff[d-a]{f}{s} \) can be written in terms of \( P_{x} \) as 
\begin{equation*}
    \ff[d-i]{f}{s}(x) = \int P_{x} f \,\d\qq[g]{\mu}{2s+1}
    \qquad \text{and} \qquad
    \ff[d-a]{f}{s}(x) = \int P_{x} f \,\d\mu.
\end{equation*}

\begin{remark}
If  \( \rho_{i} = 1 \) for all \( i \),  then \( \ff[d-i]{f}{s} = \ff[i]{f}{s} \) and \( \ff[d-a]{f}{s} = \ff[a]{f}{s} \).
\end{remark}

%We discuss the fact that interpolation at the zeros of orthogonal polynomials is equivalent to approximating the coefficients of the approximant via a Gaussian quadrature in \cref{sec:interp_gq}.

These approximations induce  \( \qq[d-i]{\Psi}{s} \) and \( \qq[d-a]{\Psi}{s} \) by
\begin{equation*}
    \int f \,\d\qq[d-i]{\Psi}{s}
    := \int \ff[d-i]{f}{s} \,\d\Psi
    \qquad\text{and}\qquad
    \int f \:  \d\qq[d-a]{\Psi}{s}
    := \int \ff[d-a]{f}{s} \,\d\Psi.
\end{equation*}
Algorithmically, this is equivalent to replacing \( m_i \) by \( \rho_{i}^{} m_i \) in the expressions for quadrature by interpolation and approximation described in \cref{sec:iq,sec:aq}.

\begin{lemma}
If \( \rho_0 = 1 \), then \( \qq[d-i]{\Psi}{s} \) and \( \qq[d-a]{\Psi}{s} \) have unit mass, and if \( P_x(y) \geq 0 \) for all \( x,y \), then \( \qq[d-i]{\Psi}{s} \) and \( \qq[d-a]{\Psi}{s} \) are weakly-increasing.
\end{lemma}

\begin{proof}
The first part of the theorem follows from the fact that
\begin{equation*}
    \int P_x \d\mu
    = \sum_{i=0}^{s} \rho_i p_i(x) \int p_i\d\mu
    = \rho_{0}.
\end{equation*}

To prove the remainder, suppose \( f \) is non-negative. 
Then clearly 
\begin{equation*}
    \ff[d-a]{f}{s}(x) = \int P_{x} f \,\d\mu
    \geq 0
\end{equation*}
so that \( \int f\,\d\qq[d-a]{\Psi}{s} \geq 0 \).
A similar argument implies \( \int f\,\d\qq[d-i]{\Psi}{s} \geq 0 \) as well.
Therefore the approximations are weakly increasing. 
\end{proof}

\begin{remark}
    While we have describe the damping procedure in terms of the damped interpolant and approximant, an equilvalent perspective is that we first smooth the distribution \( \Psi \) with the damping kernel \( P_t(y) \) and subsequently use quadrature by interpolation or approximation with this new distribution function.
\end{remark}


One particularly important damping kernel for the case \( \mu = \mu_{a,b}^T \) is given by the Jackson coefficients defined in \cref{def:jackson_coeffs}.
The associated damping kernel was used in the original proof of Jackson's theorem \cite{jackson_12} and leads to the Jackson damped KPM approximation, which is the most popular KPM variant \cite{weisse_wellein_alvermann_fehske_06,braverman_krishnan_musco_22}.
The rate of convergence of polynomial approximations using these damping coefficients is estimated in \cref{thm:damped_cheb} below.
For a discussion on other damping schemes we refer readers to \cite{weisse_wellein_alvermann_fehske_06,lin_saad_yang_16}.


\section{A priori error bounds on an interval}
\label{sec:quadrature_apriori}
%\subsection{Simple bounds from best approximation}

Most of the quadrature approximations we consider have the property that they integrate polynomials exactly. 
In this case, we can use this property to reduce error bounds to the quality of the best uniform polynomial approximations to \( f \), which we discussed in \cref{sec:uniform_poly_bounds}.


\begin{lemma}
    \label[lemma]{thm:moments_inf_norm}
    Suppose \( \Upsilon_1 \) and \( \Upsilon_2 \) are probability distribution functions whose moments are equal through degree \( s \), each constant on \( (- \infty, a) \) and \( (b,\infty) \).
    Then,
    \begin{equation*}
        \left| \int f \,\d \Upsilon_2 - \int f \,\d\Upsilon_2 \right|
         = (\TV(\Upsilon_1)+\TV(\Upsilon_2)) \min_{\deg(p)\leq s} \| f - p \|_{[a,b]}.
    \end{equation*} 
\end{lemma}

\begin{proof}%[Proof of \cref{thm:moments_inf_norm}]
    Let \( p \) be any polynomial of degree at most \( s \) and note that \( \int p \,\d\Upsilon_1 = \int p \,\d\Upsilon_2 \).
    Then, applying the triangle inequality,
    \begin{align*}
        \left| \int f \,\d \Upsilon_1 - \int f \,\d\Upsilon_2 \right|
        & = \left| \int (f - p) \,\d \Upsilon_1 - \int (f-p) \,\d\Upsilon_2 \right|
        \\& \leq \int |f - p| |\d \Upsilon_1| + \int |f - p| |\d \Upsilon_2| 
        \\& \leq \int \|f - p\|_{[a,b]} |\d \Upsilon_1| + \int \|f - p\|_{[a,b]} |\d \Upsilon_2|
        \\& = (\TV(\Upsilon_1)+\TV(\Upsilon_2)) \| f - p \|_{[a,b]}.
    \end{align*}    
    The result follows by optimizing over \( p \).
\end{proof}

\Cref{thm:moments_inf_norm} shows that the Lanczos-based Gaussian quadrature approximations always perform within a factor of two of the best polynomial approximation on \( \mathcal{I} \).
Intuitively, approaches based on explicit polynomial approximation will have error roughly equal to \( \|f-\ff{f}{s}\|_{\mathcal{I}} \) as a large portion of mass of \( \Psi \) is likely in regions where \( | f-\ff{f}{s} | \) is large.
Thus, Gaussian quadrature should not be expected to perform significantly worse than explicit polynomial methods, at least in exact arithmetic.
In fact, as we will discuss in \cref{chap:finite_precision}, even in finite preicsion arithmetic, \cref{thm:moments_inf_norm} is still morally correct.

%\subsubsection{Wasserstein approximation for damped approximant}

For some quadrature approximations we consider, polynomials are not integrated exactly.
In such cases, we turn to the following bound:
\begin{lemma}
    \label[lemma]{thm:op_inf_norm}
    Suppose \( \Upsilon_1 \) is a probability distribution function and \( \Upsilon_2 \) is defined by \( \int f \,\d\Upsilon_2 = \int \mathcal{O}[f] \d\Upsilon_1 \) for some operator \( \mathcal{O}[\:\cdot\:] \).
    Then, for any \( f \),
    \begin{equation*}
    \left| \int f \,\d\Upsilon_1
    - \int f \,\d\Upsilon_2 \right|
        = \| f - \mathcal{O}[f] \|_{[a,b]} \TV(\Upsilon_1)
    \end{equation*}
\end{lemma}

\begin{proof}%[Proof of \cref{thm:opt_inf_norm}]
The result follows by a simple application of the triangle inequality:
\begin{align*}
    \left| \int f \,\d\Upsilon_1
    - \int f \,\d\Upsilon_2 \right|
    = \left| \int ( f - \mathcal{O}[f]) \,\d\Upsilon_1 \right|
    &\leq \int \| f - \mathcal{O}[f] \|_{[a,b]} |\d\Upsilon_1|
    \\&= \| f - \mathcal{O}[f] \|_{[a,b]} \TV(\Upsilon_1).
    \tag*{\qedhere}
\end{align*}
\end{proof}

The primary downside of this bound is that it requires being able to bound \( \| f - \mathcal{O}[f] \|_{[a,b]} \).
%In some sense, this bound is less general than \cref{thm:moments_inf_norm} because it depends on the specific properties of \( \mathcal{O} \).
However, at least in the case that \( \mathcal{O} \) corresponds to undamped or Jackson damped Chebyshev interpolation or approximation, we are able to derive bounds for \( \| f - \mathcal{O}[f] \|_{[a,b]} \) directly.



\section{Qualitative comparison of algorithms}
\label{sec:QF_tradeoffs}


In \cref{sec:krylov_moments,sec:lanczos} we described \cref{alg:moments} (\ref{alg:name:moments}), \cref{alg:cheb_moments} (\ref{alg:name:cheb_moments}), \cref{alg:lanczos} (\ref{alg:name:lanczos}), \cref{alg:moments_cheb} (\ref{alg:name:moments_cheb}), and \cref{alg:moments_lanc} (\ref{alg:name:moments_lanc}) which are used to compute the information required for the quadrature approximations described in \cref{sec:quadrature}
Since \cref{alg:moments_cheb,alg:moments_lanc} respectively call \cref{alg:cheb_moments,alg:lanczos}, \cref{alg:moments,alg:cheb_moments,alg:lanczos} constitute the bulk of the computational cost of all implementations of the protoalgorithm discussed in this paper.

In each iteration, \cref{alg:moments,alg:cheb_moments,alg:lanczos} each require one matrix vector product with \( \vec{A} \) along with several scalar multiplications, vector additions, and inner products.
As such, the total computational cost of each algorithms is \( O(k(T_{\textup{mv}} + n)) \) where \( k \) is the number of iterations and \( T_{\textup{mv}} \) is the cost of a matrix-vector product with \( \vec{A} \).
Here we ignore terms depending only on \( k \) (e.g. \( k^2 \)) which are unimportant if we assume \( k\ll n \).
%\mnote{is this a safe assumption? Stefan Guettel mentioned it might not be the case in an email about a different thing}
Each of the algorithms can also be implemented using just \( O(n) \) storage; i.e. without storing the entire basis for the Krylov subspace which would cost \( O(kn) \) storage.


While the algorithms are typically quite storage efficient, there are some situations in which it may be desirable to store the whole Krylov basis.
First, \cref{alg:lanczos} is sometimes run with full reorthogonalization.
This can improve numerical stability, but increases the computation cost to \( O(k(T_{\textup{mv}} + kn)) \).
Next, by delaying all inner products to a final iteration (or using a non-blocking implementation), the number of global reductions required by \cref{alg:moments} and \cref{alg:cheb_moments} can be reduced.
Since global communication can significantly slow down Krylov subspace methods, this may speed up computation on highly parallel machines \cite{dongarra_heroux_luszczek_15,exascale}.
As mentioned earlier, there are implementations of the Lanczos algorithm which aim to decrease the number of global communications \cite{carson_demmel_15,carson_20}.
Designing Krylov subspace methods for avoiding or reducing communication costs is a large area study, but further discussion is outside the scope of this paper. 



\section{Numerical experiments}

\label{sec:numerical_experiments}

In this section, we provide a range of numerical experiments to illustrate the behavior of the algorithms described above as well as the tradeoffs between algorithms.
Our focus is primarily on quadrature approximations of the weighted CESM, as the approximation of the true CESM by the average of weighted CESMs is straightforward and well understood.

%None of the examples are meant to push the computational limits of the prototypical algorithm.

%There are many more applications than space in this paper, 


\subsection{Comparison with classical quadrature}

We begin with an example designed to illustrate some of the similarities and differences between the behavior of classical quadrature rules for continuous weight functions and the behavior of the matrix-free quadrature algorithms presented in this paper. 

\begin{figure}[htb]
    \begin{center}
        \includegraphics{imgs/ch3_runge_uniform_spec.pdf} 
    \end{center}
    \caption[{Errors for approximating \( \int f \,\d\Psi =  \vec{v}^\cT f(\vec{A}) \vec{v} \) when \( f(x) = 1/(1+16x^2) \) for a spectrum uniformly filling \( [-1,1] \).}]{%
    Errors for approximating \( \int f \,\d\Psi =  \vec{v}^\cT f(\vec{A}) \vec{v} \) when \( f(x) = 1/(1+16x^2) \) for a spectrum uniformly filling       \( [-1,1] \).
    \hspace{.25em}\emph{Legend}:
    Gaussian quadrature with 
    ({\protect\raisebox{0mm}{\protect\includegraphics[]{imgs/legend/l1.pdf}}})
    and without 
    ({\protect\raisebox{0mm}{\protect\includegraphics[]{imgs/legend/l2.pdf}}})
    reorthogonalization,
    quadrature by interpolation
    ({\protect\raisebox{0mm}{\protect\includegraphics[]{imgs/legend/l4.pdf}}}),
    and approximate quadrature by approximation
    ({\protect\raisebox{0mm}{\protect\includegraphics[]{imgs/legend/l3.pdf}}}).
    \hspace{.25em}\emph{Takeaway}: Intuition about classical approximation theory informs our understanding of algorithms for matrix-free quadrature.
    In fact, in some cases, quadrature by interpolation or approximation can provably outperform Gaussian quadrature.
    }
    \label{fig:ch3_runge_uniform_spec}
\end{figure}

\begin{figure}[htb]
    \begin{center}
        \includegraphics{imgs/ch3_runge_gap_spec.pdf} 
    \end{center}
    \caption[{Errors for approximating \( \int f \,\d\Psi =  \vec{v}^\cT f(\vec{A}) \vec{v} \) when \( f(x) = 1/(1+16x^2) \) for a spectrum uniformly filling       \( [-1,1] \) except for a gap around zero.}]{%
    Errors for approximating \( \int f \,\d\Psi =  \vec{v}^\cT f(\vec{A}) \vec{v} \) when \( f(x) = 1/(1+16x^2) \) for a spectrum uniformly filling       \( [-1,1] \) except for a gap around zero.
    \hspace{.25em}\emph{Legend}:
    Gaussian quadrature with 
    ({\protect\raisebox{0mm}{\protect\includegraphics[]{imgs/legend/l1.pdf}}})
    and without 
    ({\protect\raisebox{0mm}{\protect\includegraphics[]{imgs/legend/l2.pdf}}})
    reorthogonalization,
    quadrature by interpolation
    ({\protect\raisebox{0mm}{\protect\includegraphics[]{imgs/legend/l4.pdf}}}),
    and approximate quadrature by approximation
    ({\protect\raisebox{0mm}{\protect\includegraphics[]{imgs/legend/l3.pdf}}}).
    \hspace{.25em}\emph{Takeaway}:  
    The behavior of the algorithms are highly dependent on the eigenvalue distribution of \( \vec{A} \), and Gaussian quadrature may perform significantly better that explicit methods when the spectrum of \( \vec{A} \) has additional structure such as gaps. 
    }
    \label{fig:ch3_runge_gap_spec}
\end{figure}



Throughout this example, we use the Runge function \( f(x) = 1/(1+16x^2) \) and a vector \( \vec{v} \) with uniform weight on each eigencomponent.
%Thus, the spectral sum \( \vec{v}^\cT f[\vec{A}] \vec{v} \) can be viewed as a midpoint approximation to integrals of \( f \) over \( [-1,1] \) and \( [-1,-0.75]\cup[0.75,1] \).
We will compare the effectiveness of the Gaussian quadrature rule \( \qq[g]{\Psi}{2k-1} \), the quadrature by interpolation rule \( \qq[i]{\Psi}{2k} \) and the quadrature by approximation rule \( \qq[a]{\Psi}{2k} \). 
For the latter approximations, we set \( \mu = \mu_{-1,1}^T \), and for the quadrature by approximation rule, we use \cref{alg:aaq} with enough quadrature nodes so that the involved integrals are computed to essentially machine precision.
All three approximations can be computing using \( k \) matrix-vector products with \( \vec{A} \), and since the approaches exactly integrate polynomials of degree \( 2k-1 \) and \( 2k \) respectively, we might expect that them to behave similarly.
However, there are a variety of factors which prevent this from being the case.

In our first experiment, shown in \cref{fig:ch3_runge_uniform_spec}, the spectrum of \( \vec{A} \) uniformly fills out the interval \( [-1,1] \); i.e., \( \lambda_i = -1+(2i+1)/n \), \( i=0,1,\ldots, n-1 \).
We take \( n=10^5 \) so that \( \qq[g]{\Psi}{2k-1} \) and \( \qq[i]{\Psi}{2k} \) respectively approximate the \( k \)-point Gaussian quadrature and \( (2k-1) \)-point Fej\'{e}r quadrature rules for a uniform weight on \( [-1,1] \).
For many functions, certain quadrature by interpolation rules on \( [-1,1] \), including the Fej\'{e}r rule, behave similarly to the Gaussian quadrature rule when the same number of nodes are used \cite{trefethen_08}.
For \( f(x) = 1/(1+16x^2) \), this phenomenon is observed for some time until the convergence rate is abruptly cut in half \cite{weideman_trefethen_07}.
In our setting, a fair comparison means that the number of matrix-vector products are equal, so we see that the quadrature by interpolation approximation initially converges twice as quickly as the Gaussian quadrature approximation!
The rate of the quadrature by interpolation approximation is eventually cut in half to match the rate of the Gaussian quadrature approximation. 

\begin{figure}[htb]
    \begin{center}
        \includegraphics{imgs/ch3_model_problem.pdf} 
    \end{center}
    \caption[{Errors for approximating \( \int f \,\d\Psi =  \vec{v}^\cT f(\vec{A}) \vec{v} \) when \( f(x) = 1/x \) for model problem.}]{%
    Errors for approximating \( \int f \,\d\Psi =  \vec{v}^\cT f(\vec{A}) \vec{v} \) when \( f(x) = 1/x \) for model problem.
    \hspace{.25em}\emph{Legend}:
    Gaussian quadrature with 
    ({\protect\raisebox{0mm}{\protect\includegraphics[]{imgs/legend/l1.pdf}}})
    and without 
    ({\protect\raisebox{0mm}{\protect\includegraphics[]{imgs/legend/l2.pdf}}})
    reorthogonalization,
    quadrature by interpolation
    ({\protect\raisebox{0mm}{\protect\includegraphics[]{imgs/legend/l4.pdf}}}),
    and approximate quadrature by approximation
    ({\protect\raisebox{0mm}{\protect\includegraphics[]{imgs/legend/l3.pdf}}}).
    \hspace{.25em}\emph{Takeaway}: Without reorthogonaliztion the convergence of Gaussian quadrature is slowed. 
    However, the method still converges and can even outperform the other methods.
    }
    \label{fig:ch3_model_problem}
\end{figure}


In our second experiment, shown in \cref{fig:ch3_runge_gap_spec}, the spectrum of \( \vec{A} \) uniformly fills out the disjoint intervals \( [-1,-0.75] \cup [0.75,1] \) with the same inter-point spacing as the first example; i.e. we remove the eigenvalues in the previous example which fall between \( -0.75 \) and \( 0.75 \).
Here we observe that the Gaussian quadrature rule converges significantly faster than in the previous experiment.
This to be expected. 
Indeed, the Gaussian quadrature rule has its nodes near \( [-1,-0.75]\cup[0.75,1] \), so the union of the support of \( \Psi \) and \( \qq[i]{\Psi}{2k} \) is further from the poles of \( f \) located at \( \pm \ii /4 \).
We also note that the conditions which enabled accelerated convergence in the first experiment are no longer present, so the quadrature by interpolation approximation converges at its limiting rate \cite{trefethen_08}. 

%We conclude with the remark that \( \vec{v}^\cT f[\vec{A}] \vec{v} \) is a trapezoid rule approximation to \( \int_{-1}^{1} f(x) \,\d x \).

In the both experiments, the Lanczos based Gaussian quadrature approach behaves similar with and without reorthogonalization.
In fact, it is easily verified that the Lanczos algorithm does not lose orthogonality and behaves nearly the same regardless of whether or not reorthogonalization is used.
To the best of our knowledge, such a result has not been proved rigorously.
%Proving this phenomenon rigourously would be an interesting result.


\begin{figure}[htb]
    \begin{center}
        \includegraphics{imgs/ch3_MNIST.pdf} 
    \end{center}
    \caption[{Errors for approximating \( \int f \,\d\Psi =  \vec{v}^\cT f(\vec{A}) \vec{v} \) when \( f(x) = \bOne[x > c] \) for MNIST covariance matrix.}]{%
    Errors for approximating \( \int f \,\d\Psi =  \vec{v}^\cT f(\vec{A}) \vec{v} \) when \( f(x) = \bOne[x > c] \) for MNIST covariance matrix.
    \hspace{.25em}\emph{Legend}:
    Gaussian quadrature with 
    ({\protect\raisebox{0mm}{\protect\includegraphics[]{imgs/legend/l1.pdf}}})
    and without 
    ({\protect\raisebox{0mm}{\protect\includegraphics[]{imgs/legend/l2.pdf}}})
    reorthogonalization,
    quadrature by interpolation
    ({\protect\raisebox{0mm}{\protect\includegraphics[]{imgs/legend/l4.pdf}}}),
    and approximate quadrature by approximation
    ({\protect\raisebox{0mm}{\protect\includegraphics[]{imgs/legend/l3.pdf}}}).
    \hspace{.25em}\emph{Takeaway}: Without reorthogonaliztion the convergence of Gaussian quadrature is slowed. 
    However, the method still converges and can even outperform the other methods.
    }
    \label{fig:ch3_California}
\end{figure}



\subsection{Finite precision convergence}

In this example, we consider several experiments where orthogonality is lost and the effects of finite precision arithmetic are easily observed.
In both experiments we use diagonal matrices scaled so \( \| \vec{A} \|_2 = 1 \) and set \( \vec{v} \) to have uniform entries.
We set \( a,b \) as the largest and smallest eigenvalues respectively and again use \( \mu = \mu_{a,b}^T \) for the interpolatory and quadrature by approximations.


In the first experiment, shown in \cref{fig:ch3_model_problem}, the eigenvalues of \( \vec{A} \) are distributed according to the model problem \cref{eqn:model_problem} and \( f(x) = 1/x \).
Specifically, the eigenvalues are given by the model problem with selected parameters \( n = 300 \), \( \kappa = 10^3 \), and \( \rho = 0.85 \).
In the second experiment, shown in the right panel \cref{fig:ch3_California}, we use the \( n=9664 \) eigenvalues of the California matrix from the sparse matrix suite \cite{davis_hu_11} and the function \( f(x) = |x| \).


In both cases, the Jacobi matrices produced by Lanczos, with or without reorthogonalization, differ greatly; i.e. the difference of the matrices is on the order of \( \| \vec{A} \|_2 \). 
Even so, the modified moments for \( \mu = \mu_{a,b}^T \) obtained by \cref{alg:moments_cheb,alg:moments_lanc} differ only in the 12th digit and 14th digits respectively.
Using one approach in place of the other does not noticeably impact the convergence of the quadrature by interpolation approximations.




